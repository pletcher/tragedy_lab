{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, MiniBatchNMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "batch_size = 128\n",
    "init = \"nndsvda\"\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('grc_perseus_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5322694",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = by_play_and_speaker.select(pl.col('text').list.join(\" \"))\n",
    "raw_samples = data['text'].to_list()\n",
    "\n",
    "data_samples = []\n",
    "\n",
    "STOPS = [\"δέ\", \"τε\", \"ἀλλ\", \"ἀλλά\", \"οὔτε\"]\n",
    "\n",
    "with nlp.select_pipes(enable=\"lemmatizer\"):\n",
    "    for row in raw_samples:\n",
    "        doc = nlp(row)\n",
    "        lemmata = [t.lemma_ for t in doc]\n",
    "\n",
    "        data_samples.append(' '.join([t.lemma_ for t in doc if not t.is_stop and t.lemma_ not in STOPS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\n",
    "    \"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "    \"n_samples=%d and n_features=%d...\" % (n_samples, n_features)\n",
    ")\n",
    "t0 = time()\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1,\n",
    "    init=init,\n",
    "    beta_loss=\"frobenius\",\n",
    "    alpha_W=0.00005,\n",
    "    alpha_H=0.00005,\n",
    "    l1_ratio=1,\n",
    ").fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\n",
    ")\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
    "    \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "t0 = time()\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1,\n",
    "    init=init,\n",
    "    beta_loss=\"kullback-leibler\",\n",
    "    solver=\"mu\",\n",
    "    max_iter=1000,\n",
    "    alpha_W=0.00005,\n",
    "    alpha_H=0.00005,\n",
    "    l1_ratio=0.5,\n",
    ").fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf,\n",
    "    tfidf_feature_names,\n",
    "    n_top_words,\n",
    "    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
    ")\n",
    "\n",
    "# Fit the MiniBatchNMF model\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf \"\n",
    "    \"features, n_samples=%d and n_features=%d, batch_size=%d...\"\n",
    "    % (n_samples, n_features, batch_size),\n",
    ")\n",
    "t0 = time()\n",
    "mbnmf = MiniBatchNMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1,\n",
    "    batch_size=batch_size,\n",
    "    init=init,\n",
    "    beta_loss=\"frobenius\",\n",
    "    alpha_W=0.00005,\n",
    "    alpha_H=0.00005,\n",
    "    l1_ratio=0.5,\n",
    ").fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    mbnmf,\n",
    "    tfidf_feature_names,\n",
    "    n_top_words,\n",
    "    \"Topics in MiniBatchNMF model (Frobenius norm)\",\n",
    ")\n",
    "\n",
    "# Fit the MiniBatchNMF model\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting the MiniBatchNMF model (generalized Kullback-Leibler \"\n",
    "    \"divergence) with tf-idf features, n_samples=%d and n_features=%d, \"\n",
    "    \"batch_size=%d...\" % (n_samples, n_features, batch_size),\n",
    ")\n",
    "t0 = time()\n",
    "mbnmf = MiniBatchNMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1,\n",
    "    batch_size=batch_size,\n",
    "    init=init,\n",
    "    beta_loss=\"kullback-leibler\",\n",
    "    alpha_W=0.00005,\n",
    "    alpha_H=0.00005,\n",
    "    l1_ratio=0.5,\n",
    ").fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    mbnmf,\n",
    "    tfidf_feature_names,\n",
    "    n_top_words,\n",
    "    \"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
